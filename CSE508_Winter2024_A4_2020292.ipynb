{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8172227,"sourceType":"datasetVersion","datasetId":4836761},{"sourceId":8172260,"sourceType":"datasetVersion","datasetId":4836785}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T05:08:44.117121Z","iopub.execute_input":"2024-04-22T05:08:44.118348Z","iopub.status.idle":"2024-04-22T05:08:57.508403Z","shell.execute_reply.started":"2024-04-22T05:08:44.118300Z","shell.execute_reply":"2024-04-22T05:08:57.507147Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch.optim as optim\nimport seaborn as sns\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:09:07.623966Z","iopub.execute_input":"2024-04-22T05:09:07.624471Z","iopub.status.idle":"2024-04-22T05:09:07.638178Z","shell.execute_reply.started":"2024-04-22T05:09:07.624388Z","shell.execute_reply":"2024-04-22T05:09:07.637104Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:11:34.784624Z","iopub.execute_input":"2024-04-22T05:11:34.785125Z","iopub.status.idle":"2024-04-22T05:11:34.794626Z","shell.execute_reply.started":"2024-04-22T05:11:34.785091Z","shell.execute_reply":"2024-04-22T05:11:34.793463Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"markdown","source":"**Loading the finetuned model**","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/fine-tuned-gpt\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/tokenizer-fine-tuned\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:11:37.555213Z","iopub.execute_input":"2024-04-22T05:11:37.555643Z","iopub.status.idle":"2024-04-22T05:11:38.347825Z","shell.execute_reply.started":"2024-04-22T05:11:37.555608Z","shell.execute_reply":"2024-04-22T05:11:38.346577Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\nlearning_rate = 3e-4\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:11:42.074597Z","iopub.execute_input":"2024-04-22T05:11:42.075043Z","iopub.status.idle":"2024-04-22T05:11:42.088527Z","shell.execute_reply.started":"2024-04-22T05:11:42.075014Z","shell.execute_reply":"2024-04-22T05:11:42.087272Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def topk(probs, n=9):\n    # The scores are initially softmaxed to convert to probabilities\n    probs = torch.softmax(probs, dim= -1)\n\n    # PyTorch has its own topk method, which we use here\n    tokensProb, topIx = torch.topk(probs, k=n)\n\n    # The new selection pool (9 choices) is normalized\n    tokensProb = tokensProb / torch.sum(tokensProb)\n\n    # Send to CPU for numpy handling\n    tokensProb = tokensProb.cpu().detach().numpy()\n\n    # Make a random choice from the pool based on the new prob distribution\n    choice = np.random.choice(n, 1, p = tokensProb)\n    tokenId = topIx[choice][0]\n\n    return int(tokenId)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:11:44.769173Z","iopub.execute_input":"2024-04-22T05:11:44.781536Z","iopub.status.idle":"2024-04-22T05:11:44.904315Z","shell.execute_reply.started":"2024-04-22T05:11:44.780437Z","shell.execute_reply":"2024-04-22T05:11:44.882538Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def model_infer(model, tokenizer, review, max_length=15):\n    # Preprocess the init token (task designator)\n    review_encoded = tokenizer.encode(review)\n    result = review_encoded\n    initial_input = torch.tensor(review_encoded).unsqueeze(0).to(device)\n\n    with torch.set_grad_enabled(False):\n        # Feed the init token to the model\n        output = model(initial_input)\n\n        # Flatten the logits at the final time step\n        logits = output.logits[0,-1]\n\n        # Make a top-k choice and append to the result\n        result.append(topk(logits))\n\n        # For max_length times:\n        for _ in range(max_length):\n            # Feed the current sequence to the model and make a choice\n            input = torch.tensor(result).unsqueeze(0).to(device)\n            output = model(input)\n            logits = output.logits[0,-1]\n            res_id = topk(logits)\n\n            # If the chosen token is EOS, return the result\n            if res_id == tokenizer.eos_token_id:\n                return tokenizer.decode(result)\n            else: # Append to the sequence\n                result.append(res_id)\n    # IF no EOS is generated, return after the max_len\n    return tokenizer.decode(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:11:47.557541Z","iopub.execute_input":"2024-04-22T05:11:47.558330Z","iopub.status.idle":"2024-04-22T05:11:47.567634Z","shell.execute_reply.started":"2024-04-22T05:11:47.558284Z","shell.execute_reply":"2024-04-22T05:11:47.566242Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"review_input = input(\"Enter a review\")\nsummary_input = input(\"Enter the summary of review\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:11:51.153726Z","iopub.execute_input":"2024-04-22T05:11:51.154163Z","iopub.status.idle":"2024-04-22T05:12:15.854823Z","shell.execute_reply.started":"2024-04-22T05:11:51.154132Z","shell.execute_reply":"2024-04-22T05:12:15.853608Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a review \"The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.\"\nEnter the summary of review \"Good for beginners but has tuning stability issues.\"\n"}]},{"cell_type":"code","source":"summary = model_infer(model, tokenizer, review_input+ \" TL;DR \").split(\" TL;DR \")[1].strip()\nprint(\"Generated Summary: \", summary)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:12:20.215281Z","iopub.execute_input":"2024-04-22T05:12:20.215698Z","iopub.status.idle":"2024-04-22T05:12:35.637636Z","shell.execute_reply.started":"2024-04-22T05:12:20.215666Z","shell.execute_reply":"2024-04-22T05:12:35.636263Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2024-04-22 05:12:25.219043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 05:12:25.219196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 05:12:25.382249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Generated Summary:  The Fender CD-100S Dreadnought Ac Guitar\n","output_type":"stream"}]},{"cell_type":"code","source":"hypothesis = summary\nreference = summary_input","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:12:40.867934Z","iopub.execute_input":"2024-04-22T05:12:40.868344Z","iopub.status.idle":"2024-04-22T05:12:40.873860Z","shell.execute_reply.started":"2024-04-22T05:12:40.868314Z","shell.execute_reply":"2024-04-22T05:12:40.872529Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Calculating ROGUE Scores**","metadata":{}},{"cell_type":"code","source":"def calculate_rouge_precision(hypothesis, reference):\n    common_tokens = set(hypothesis) & set(reference)\n    return len(common_tokens) / len(hypothesis)\n\ndef calculate_rouge_recall(hypothesis, reference):\n    common_tokens = set(hypothesis) & set(reference)\n    return len(common_tokens) / len(reference)\n\ndef calculate_rouge_f1_score(precision, recall):\n    if precision + recall == 0:\n        return 0\n    return 2 * (precision * recall) / (precision + recall)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:12:43.577304Z","iopub.execute_input":"2024-04-22T05:12:43.577723Z","iopub.status.idle":"2024-04-22T05:12:43.586718Z","shell.execute_reply.started":"2024-04-22T05:12:43.577692Z","shell.execute_reply":"2024-04-22T05:12:43.584996Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"rouge_1_precision = calculate_rouge_precision(hypothesis, reference)\nrouge_1_recall = calculate_rouge_recall(hypothesis, reference)\nrouge_1_f1_score = calculate_rouge_f1_score(rouge_1_precision, rouge_1_recall)\n\nhypothesis_bigrams = set(zip(hypothesis, hypothesis[1:]))\nreference_bigrams = set(zip(reference, reference[1:]))\n\nrouge_2_precision = calculate_rouge_precision(hypothesis_bigrams, reference_bigrams)\nrouge_2_recall = calculate_rouge_recall(hypothesis_bigrams, reference_bigrams)\nrouge_2_f1_score = calculate_rouge_f1_score(rouge_2_precision, rouge_2_recall)\n\n\n# Longest Common Subsequence (LCS) for ROUGE-L\ndef lcs(X, Y):\n    m = len(X)\n    n = len(Y)\n\n    # Create a table to store lengths of longest common suffixes of substrings\n    # Note that LCSuff[i][j] contains length of longest common suffix of X[0..i-1] and Y[0..j-1].\n    LCSuff = [[0] * (n + 1) for _ in range(m + 1)]\n\n    # To store the length of the longest common substring\n    length = 0\n\n    # To store the index of the cell which contains the maximum value.\n    # This will be used to print the substring.\n    row, col = 0, 0\n\n    # Following steps build LCSuff[m+1][n+1] in bottom-up fashion.\n    for i in range(m + 1):\n        for j in range(n + 1):\n            if i == 0 or j == 0:\n                LCSuff[i][j] = 0\n            elif X[i - 1] == Y[j - 1]:\n                LCSuff[i][j] = LCSuff[i - 1][j - 1] + 1\n                if LCSuff[i][j] > length:\n                    length = LCSuff[i][j]\n                    row = i\n                    col = j\n            else:\n                LCSuff[i][j] = 0\n\n    # If we have non-zero length, then LCSuff contains the longest common substring.\n    if length != 0:\n        lcs_str = X[row - length: row]\n    else:\n        lcs_str = []\n\n    return len(lcs_str) / len(X)\n\nrouge_l_precision = calculate_rouge_precision(hypothesis, reference)\nrouge_l_recall = calculate_rouge_recall(hypothesis, reference)\nrouge_l_f1_score = calculate_rouge_f1_score(rouge_l_precision, rouge_l_recall)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:12:46.560171Z","iopub.execute_input":"2024-04-22T05:12:46.561004Z","iopub.status.idle":"2024-04-22T05:12:46.576324Z","shell.execute_reply.started":"2024-04-22T05:12:46.560968Z","shell.execute_reply":"2024-04-22T05:12:46.574956Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(\"ROUGE-1: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(rouge_1_precision, rouge_1_recall, rouge_1_f1_score))\nprint(\"ROUGE-2: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(rouge_2_precision, rouge_2_recall, rouge_2_f1_score))\nprint(\"ROUGE-L: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(rouge_l_precision, rouge_l_recall, rouge_l_f1_score))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T05:12:52.109560Z","iopub.execute_input":"2024-04-22T05:12:52.110269Z","iopub.status.idle":"2024-04-22T05:12:52.116932Z","shell.execute_reply.started":"2024-04-22T05:12:52.110234Z","shell.execute_reply":"2024-04-22T05:12:52.115501Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"ROUGE-1: Precision: 0.33, Recall: 0.25, F1-Score: 0.28\nROUGE-2: Precision: 0.13, Recall: 0.10, F1-Score: 0.11\nROUGE-L: Precision: 0.33, Recall: 0.25, F1-Score: 0.28\n","output_type":"stream"}]}]}